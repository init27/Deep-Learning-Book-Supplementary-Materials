{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data. Formally, PCA is a statistical technique for reducing the dimensionality of a dataset. This is accomplished by linearly transforming the data into a new coordinate system where (most of) the variation in the data can be described with fewer dimensions than the initial data.\n",
    "\n",
    "[Reference](https://en.wikipedia.org/wiki/Principal_component_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: \n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]], dtype=torch.float64)\n",
      "\n",
      "U: \n",
      "tensor([[ 0.6708,  0.0393,  0.2236],\n",
      "        [ 0.2236,  0.7060, -0.6708],\n",
      "        [-0.2236,  0.7060,  0.6708],\n",
      "        [-0.6708,  0.0393, -0.2236]], dtype=torch.float64)\n",
      "\n",
      "S: \n",
      "tensor([11.6190,  0.0000,  0.0000], dtype=torch.float64)\n",
      "\n",
      "V: \n",
      "tensor([[-0.5774,  0.0000,  0.8165],\n",
      "        [-0.5774, -0.7071, -0.4082],\n",
      "        [-0.5774,  0.7071, -0.4082]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(12, dtype=float).view(4, 3)\n",
    "\n",
    "U, S, V = torch.pca_lowrank(A)\n",
    "\n",
    "print(f\"Matrix A: \\n{A}\")\n",
    "print(f\"\\nU: \\n{U}\")\n",
    "print(f\"\\nS: \\n{S}\")\n",
    "print(f\"\\nV: \\n{V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation of `(U, S, V)` to PCA is as follows:\n",
    "\n",
    "- `A` is a data matrix with `m` samples and `n` features\n",
    "\n",
    "- the `V` columns represent the principal directions\n",
    "\n",
    "- `S ** 2 / (m - 1)` contains the eigenvalues of `A^T A / (m - 1)` which is the covariance of `A` when `center=True` is provided.\n",
    "\n",
    "- `matmul(A, V[:, :k])` projects data to the first `k` principal components\n",
    "\n",
    "[Source](https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Resources**\n",
    "- [MIT Lecture](https://www.youtube.com/watch?v=WW3ZJHPwvyg)\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fbb7bc7656f6be0ea30baeef166ae9c5110869b59dc75fa6c387b1b2aec8c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
