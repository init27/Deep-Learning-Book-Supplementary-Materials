{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook will cover the definitions, numpy and torch equivalent implementation of Probability and Information theory from MIT's deep learning book. \n\n\n1. [Study group](http://www.youtube.com/watch?v=Db7B8yBAnHQ)\n2. [Book](http://www.deeplearningbook.org/)\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport math","metadata":{"execution":{"iopub.status.busy":"2022-12-28T12:11:08.715067Z","iopub.execute_input":"2022-12-28T12:11:08.715593Z","iopub.status.idle":"2022-12-28T12:11:11.298613Z","shell.execute_reply.started":"2022-12-28T12:11:08.715479Z","shell.execute_reply":"2022-12-28T12:11:11.297123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Probability \nThe study of uncertainty is known as probability.\n1. We can use probability to represent degree of belief  e.g. Doctors diagnose a patient with a disease using certain uncertainty 1 being absolutely certain and 0 otherwise.\n2. We can use probability to denote the rate at which an event is likely to occur e.g. Probabilty that a die will land 6 is 1/6, probability that a coin will land heads is 0.5. \n\nTo work with probability we need two pieces of information: The random variable and the probability distribution.","metadata":{}},{"cell_type":"markdown","source":"## Random variable\n\nA random variable is a variable that can take on diÔ¨Äerent values randomly. Random variables are a description of a possible states given a probability distribution. The probability distribution tells how likely a state will occur. Random variable can be discrete or continuous. $\\text{x}$ denotes a random variable and $x$ denotes it's value. ","metadata":{}},{"cell_type":"markdown","source":"## Probability distribution\n\nA probability distribution is a description of how likely a random variable or set of random variables is to take on each of its possible states. The description of a probabiltiy distribution depends on whether the variables are discrete or continuous. ","metadata":{}},{"cell_type":"markdown","source":"## Discrete variables and probability mass functions (PMF)\nThe probability mass function(PMF) maps from a state of a random variable to the probability of that random variable taking on that state. The probability that $\\text{x} = x$ is denoted as $P (x)$, with a probability of 1 indicating that $\\text{x} = x$ is certain and a probability of 0 indicating that $\\text{x} = x$ is impossible. \n\n1. The distributions are usually written in this form: $\\text{x}\\sim P(\\text{x})$\n2. When PMFs are used on over many variables it is called joint probability distribution. $P(\\text{x}=x,\\text{y}=y)$\n\nPMFs must satisfy the following properties\n* The domain of $P$ must be the set of all possible states of x.\n* $\\forall{x} \\in \\text{x}, 0 \\leq P(x) \\leq 1$. That means that probality of a given state must be greater than equal to 0 or less than equal to 1.\n* $\\sum_{\\forall{x} \\in \\text{x}} P(x)=1$. The sum of probabilites of all the states must be equal to 1. ","metadata":{}},{"cell_type":"markdown","source":"## Continous variables and probability dense functions (PDF)\nA probability density function (PDF) is used to define the random variable‚Äôs probability coming within a distinct range of values, as opposed to taking on any one value. It is denoted as $p(x)$.It must satisfy the following conditions:\n\n* The domain of  ùëÉ  must be the set of all possible states of x.\n* $\\forall{x} \\in \\text{x}, p(x) \\geq 0$\n* $\\int p(x)dx=1$\n\nSince PDF doesn't provide probability for a distinct value, we have to find probability over a range of values which can done by integrating over the range in PDF function. $\\int_{[a,b]}p(x)dx$","metadata":{}},{"cell_type":"markdown","source":"## Marginal probability\nWhen we have a probability distribtution over a set of variables, the marginal probability is the probability distribution over a subset.\n\nE.g.\n\nIf we have random variables $\\text{x}$ and $\\text{y}$ and $P(\\text{x},\\text{y})$ is known then $P(\\text(x))$ can be calculated using the sum rule.\n$$\n\\forall x \\in \\text{x}, P(\\text{x} = x) = \\sum_{y}P(\\text{x} = x, \\text{y} = y)\n$$\nFor continuous variable\n$$\np(x) = \\int p(x,y)dx\n$$","metadata":{}},{"cell_type":"markdown","source":"## Conditional probability\nConditional probability is the probability of an event, given that another event has occured. It is calculated using this formula only when $P(\\text{x}=x) > 0$.\n$$\nP(\\text{y} = y | \\text{x} = x) = \\frac{P(\\text{y}=y,\\text{x}=x)}{P(\\text{x}=x)}\n$$\n\n","metadata":{}},{"cell_type":"markdown","source":"## Independence and conditional indepdence\n","metadata":{}},{"cell_type":"markdown","source":"Two random variables x and y are independent if their probability distribution can be expressed as a product of two factors, one involving only x and one involving only y:\n$$\n\\forall x \\in \\text{x},y \\in \\text{y}, p(\\text{x} = x, \\text{y} = y) = p(\\text{x} = x)p(\\text{y} = y)\n$$\nTwo random variables x and y are conditionally independent given a random variable z if the conditional probability distribution over x and y factorizes in this way for every value of z:\n$$\n\\forall x \\in \\text{x},y \\in \\text{y}, z \\in \\text{z}, p(\\text{x} = x, \\text{y} = y | \\text{z} = z) = p(\\text{x} = x | \\text{z} = z)p(\\text{y} = y | \\text{z} = z))\n$$","metadata":{}},{"cell_type":"markdown","source":"## Expectation, Variance and Covariance\n\nThe expectation or expected value of some function $f(x)$ with respect to a probability distribution $P (x)$ is the average or mean value that $f$ takes on when $x$ is drawn from $P$. For discrete variables:\n$$\n\\mathbb{E}_{\\text{x}\\sim P}[f(x)] = \\sum_{x}P(x)f(x)\n$$\nFor continuous variables:\n$$\n\\mathbb{E}_{\\text{x}\\sim p}[f(x)] = \\int p(x)f(x)dx\n$$\nThe variance gives a measure of how much the values of a function of a random variable x vary as we sample diÔ¨Äerent values of x from its probability distribution. If the variance is low the values of f(x) are near their expected value. The square root of variance is standard deviation\n$$\nVar(f(x))= \\mathbb{E}[(f(x) - \\mathbb{E}[f(x)])^2]\n$$\nThe covariance gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:\n$$\nCov(f(x),g(x))=\\mathbb{E}[(f(x) - \\mathbb{E}[f(x)])](g(y) - \\mathbb{E}[g(y)]]\n$$\n* High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time.\n* If the sign of the covariance is positive, then both variables tend to take on relatively high values simultaneously.\n* If the sign of the covariance is negative, then one variable tends to take on a relatively high value at the times that the other takes on a relatively low value and vice versa\n","metadata":{}},{"cell_type":"markdown","source":"# Common probability distribution\n\n## Bernouli distribution\nThe Bernoulli distribution is a distribution over a single binary random variable. It is controlled by a single parameter $\\phi \\in [0, 1]$, which gives the probability of the random variable being equal to 1.\n$$\nP(\\text{x}=x)= \\phi^{x} (1-\\phi)^{1-x}\\\\\n\\mathbb{E}_{x}[x] = \\phi\\\\\nVar_{x}(x)=\\phi(1-\\phi)\n$$\n## Implementation","metadata":{}},{"cell_type":"code","source":"# Torch\nbernouli = torch.distributions.bernoulli.Bernoulli(probs=(0.7))\nprint(f\"Bernouli sample: {bernouli.sample((1,))}\")\n\n# Numpy \nbernouli_numpy = np.random.binomial(1,0.5,)\nprint(f'\\nBernouli sample numpy: {bernouli_numpy}')\n","metadata":{"execution":{"iopub.status.busy":"2022-12-25T12:46:59.691179Z","iopub.execute_input":"2022-12-25T12:46:59.691552Z","iopub.status.idle":"2022-12-25T12:46:59.698689Z","shell.execute_reply.started":"2022-12-25T12:46:59.691519Z","shell.execute_reply":"2022-12-25T12:46:59.697164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multinomial distribution\n\nA multinomial distribution is the distribution over vectors in $(0, . . . , n)^{k}$ representing how many times each of the k categories is visited when n samples are drawn from a multinoulli distribution. The book focuses on multinouli distribution which is a special case of multinomial distribution when $n=1$.\n\n## Implementation\nThe torch output shows all the 20 samples. While the numpy output shows how many times an even occured e.g the first event at index 0 occured 7 times if the output is [7,3,8,2]","metadata":{}},{"cell_type":"code","source":"#Torch\nmultinomial = torch.distributions.categorical.Categorical(probs=torch.Tensor([0.4,0.2,0.3,0.1]))\nprint(f\"Multinomial sample: {multinomial.sample((20,))}\")\n\n# Numpy\nmultinomial_numpy =  np.random.multinomial(n=20, pvals=[0.4,0.2,0.3,0.1])\nprint(f\"Multinomial numpy sample: {multinomial_numpy}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-25T13:38:45.100115Z","iopub.execute_input":"2022-12-25T13:38:45.101131Z","iopub.status.idle":"2022-12-25T13:38:45.108938Z","shell.execute_reply.started":"2022-12-25T13:38:45.101094Z","shell.execute_reply":"2022-12-25T13:38:45.107322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gaussian distribution\nThe most commonly used distribution that anyone will encounter is the normal distribution which is also known as Gaussian distribution.\n$$\nN(x,\\mu,\\sigma^{2}) = \\sqrt{\\frac{1}{2\\pi\\sigma^{2}}}(-\\frac{1}{2\\sigma^{2}}(x-\\mu)^{2})\n$$\nThe two parameters $\\mu \\in \\mathbb{R}$ and $\\sigma \\in (0, \\inf)$ control the normal distribution. The parameter $\\mu$ gives the coordinate of the central peak. This is also the mean of the distribution: $\\mathbb{E}[x] = \\mu$. The standard deviation of the distribution is given by $\\sigma$, and the variance by $\\sigma^{2}$.\n","metadata":{}},{"cell_type":"markdown","source":"## Bell curve","metadata":{}},{"cell_type":"code","source":"mu = 0\nvariance = 1\nsigma = math.sqrt(variance)\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\nplt.plot(x, stats.norm.pdf(x, mu, sigma))\nplt.title('Bell curve')\nplt.ylabel('p(x)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-25T13:26:20.504672Z","iopub.execute_input":"2022-12-25T13:26:20.505043Z","iopub.status.idle":"2022-12-25T13:26:20.883386Z","shell.execute_reply.started":"2022-12-25T13:26:20.505009Z","shell.execute_reply":"2022-12-25T13:26:20.882410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementation","metadata":{}},{"cell_type":"code","source":"# Torch\nt_n = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\nprint(f\"Sample: {t_n.sample((20,))}\")\n\n# Numpy\nt_num = np.random.normal(loc=0.0,scale=1.0,size=(20,))\nprint(f\"\\nSample: {t_num}\")","metadata":{"execution":{"iopub.status.busy":"2022-12-25T13:38:21.743143Z","iopub.execute_input":"2022-12-25T13:38:21.743960Z","iopub.status.idle":"2022-12-25T13:38:21.751270Z","shell.execute_reply.started":"2022-12-25T13:38:21.743925Z","shell.execute_reply":"2022-12-25T13:38:21.750340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exponential and Laplace distributions\nExponential distribution has a sharp point at $x=0$\n$$\np(x;\\lambda) = \\lambda 1_{x \\geq 0}exp(-\\lambda x)\n$$\nThe exponential distribution uses the indicator function $1_{x \\geq 0}$ to assign probability\nzero to all negative values of $x$\n\nLaplace distribution has a sharp peak of probability at $\\mu$\n$$\nLaplace(x;\\mu,\\gamma) = \\frac{1}{2\\gamma}exp(-\\frac{|x-\\mu|}{\\gamma})\n$$\n![Laplace distribution](http://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Laplace_pdf_mod.svg/1280px-Laplace_pdf_mod.svg.png)\n## Implementation","metadata":{}},{"cell_type":"code","source":"# Torch\nm = torch.distributions.exponential.Exponential(torch.tensor([1.0]))\nprint(f\"Sample: {m.sample((20,))}\\n\")\n\n#loc is mean scale is std\nm = torch.distributions.laplace.Laplace(loc=torch.tensor([0.0]), scale=torch.tensor([1.0]))\nprint(f\"Sample: {m.sample((20,))}\\n\")\n\n\n# Numpy\nm = np.random.exponential(scale=1.0)\nprint(f\"Sample: {m}\\n\")\n\nm = np.random.laplace(loc=0.0, scale=1.0)\nprint(f\"Sample: {m}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-12-25T14:45:40.606436Z","iopub.execute_input":"2022-12-25T14:45:40.606864Z","iopub.status.idle":"2022-12-25T14:45:40.618135Z","shell.execute_reply.started":"2022-12-25T14:45:40.606829Z","shell.execute_reply":"2022-12-25T14:45:40.617102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dirac and Empirical distributions\n\nDirac is a type of probability distribution where all the mass clusters around a single point. It is defined using the following function:\n$$\np(x)=\\lambda(x-\\mu)\n$$\nHere the mass is $\\lambda$ shifted by $\\mu$ so in the diagram you can see that there is a peak of probability mass at $x=\\mu$\nThe Dirac delta function is deÔ¨Åned such that it is zero-valued everywhere except 0, yet integrates to 1. Here is a diagram showing the dirac function.\n[Dirac function](http://en.wikipedia.org/wiki/Dirac_delta_function#/media/File:Dirac_distribution_PDF.svg.png).\n\nIn the book it mentions that the dirac delta function is a genralized function meaning that it doesn't associate a real value with each value $x$. It puts less mass on all other points except 0.\n\nDirac delta distribution is used as a component of empirical distribution\n$$\n\\hat{p}(x) = \\frac{1}{m}\\sum_{i=1}^{m} \\lambda (x-x^{(i)})\n$$\nIn this equation, each points from $x^{(1)}, ..., x^{(m)}$ has a probability mass of $\\frac{1}{m}$. In case of discrete variables, the probability of each state is basically the frequncy of that state in the training set.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Common functions\n## Sigmoid\n$$\n\\sigma(x) = \\frac{1}{1+exp(-x)}\n$$\n","metadata":{}},{"cell_type":"code","source":"# Torch\nt = torch.arange(-10,10,0.2)\ns = torch.special.expit(t)\nplt.plot(t,s)\nplt.ylabel('ùúé(x)')","metadata":{"execution":{"iopub.status.busy":"2022-12-28T12:19:10.538784Z","iopub.execute_input":"2022-12-28T12:19:10.539195Z","iopub.status.idle":"2022-12-28T12:19:10.778812Z","shell.execute_reply.started":"2022-12-28T12:19:10.539163Z","shell.execute_reply":"2022-12-28T12:19:10.777468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Softplus\n\n$$\n\\zeta(x)=log(1+exp(x))\n$$","metadata":{}},{"cell_type":"code","source":"# Torch\nt = torch.arange(-10,10,0.2)\nSoftplus = torch.nn.Softplus()\ns = Softplus(t)\n\nplt.plot(t,s)\nplt.ylabel('ùúÅ(ùë•)')","metadata":{"execution":{"iopub.status.busy":"2022-12-28T12:19:01.663056Z","iopub.execute_input":"2022-12-28T12:19:01.663489Z","iopub.status.idle":"2022-12-28T12:19:01.887140Z","shell.execute_reply.started":"2022-12-28T12:19:01.663453Z","shell.execute_reply":"2022-12-28T12:19:01.886200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}